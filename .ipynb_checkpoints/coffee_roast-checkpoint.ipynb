{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c23dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Normalization,Dense\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from load_coffee_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a5dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78aba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba264d",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be65474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = Normalization(axis=-1)\n",
    "\n",
    "#'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
    "norm_layer.adapt(x_train)  # learns mean, variance\n",
    "\n",
    "x_norm = norm_layer(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb109c4",
   "metadata": {},
   "source": [
    "copy our data to increase the training set size and reduce the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5015af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tile = np.tile(x_norm, (100,1))\n",
    "y_tile = np.tile(y_train, (100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baaba78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2) (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_tile.shape, y_tile.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a5bc8",
   "metadata": {},
   "source": [
    "### Creating a model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0981aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "\n",
    "model = Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(2,)),\n",
    "            Dense(units=3, activation=\"sigmoid\", name=\"layer1\"),\n",
    "            Dense(units=1, activation=\"sigmoid\", name=\"layer2\")          \n",
    "        ]   \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e140d8a",
   "metadata": {},
   "source": [
    "The model.summary() provides a description of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c57bb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6c007",
   "metadata": {},
   "source": [
    "Instantiated weights and biases by Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bbfaf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 0.14331901 -0.3954503   0.5523132 ]\n",
      " [-0.5084111   0.53487957 -0.81892145]] b: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "print(f\"w: {W1} b: {b1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3826adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[-1.1920437 ]\n",
      " [ 0.9825512 ]\n",
      " [ 0.42753232]] b: [0.]\n"
     ]
    }
   ],
   "source": [
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"w: {W2} b: {b2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa24f40",
   "metadata": {},
   "source": [
    "The model.compile statement defines a loss function and specifies a compile optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b66e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d89ddf",
   "metadata": {},
   "source": [
    "The model.fit statement runs gradient descent and fits the weights to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19bfbd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0027\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0026\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0026\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0025\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0024\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0024\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.0023\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0021\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0021\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22654b5f8b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_tile, \n",
    "         y_tile,\n",
    "         epochs=10   # number of iterations on complete dataset\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db04b1",
   "metadata": {},
   "source": [
    "### Epochs and batches\n",
    "The number of epochs was set to 10. This specifies that the entire data set should be applied during training 10 times. During training, you see output describing the progress of training that looks like this:\n",
    "\n",
    "Epoch 1/10\n",
    "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
    "The first line, Epoch 1/10, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line 6250/6250 [==== is describing which batch has been executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbabcd2",
   "metadata": {},
   "source": [
    "Updated weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469edeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 14.214207   -10.640017     0.05994322]\n",
      " [ 11.904436    -0.24709323  10.101967  ]] b: [  1.838358 -11.545151  12.142668]\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "print(f\"w: {w1} b: {b1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cad2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 14.214207   -10.640017     0.05994322]\n",
      " [ 11.904436    -0.24709323  10.101967  ]] b: [  1.838358 -11.545151  12.142668]\n"
     ]
    }
   ],
   "source": [
    "w2, b2 = model.get_layer(\"layer1\").get_weights()\n",
    "print(f\"w: {w2} b: {b2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cb549",
   "metadata": {},
   "source": [
    "Different training runs can produce somewhat different (w and b) hence different results\n",
    "- so we will load some saved weights from a previous training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fb028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([\n",
    "    [-8.94,  0.29, 12.89],\n",
    "    [-0.17, -7.34, 10.79]] )\n",
    "b1 = np.array([-9.87, -9.28,  1.01])\n",
    "W2 = np.array([\n",
    "    [-31.38],\n",
    "    [-27.86],\n",
    "    [-32.79]])\n",
    "b2 = np.array([15.54])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb5e9c",
   "metadata": {},
   "source": [
    "#### to set weight and bias to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "696d340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(\"layer1\").set_weights([W1, b1])\n",
    "model.get_layer(\"layer2\").set_weights([W2, b2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f915394",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2074384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test data\n",
    "X_test = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17]])   # negative example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385fbc4",
   "metadata": {},
   "source": [
    "we have normalized the input features so we must normalize our test data as well.\n",
    "To make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d00f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = norm_layer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "000696d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.6204257e-01],\n",
       "       [3.0316290e-08]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting \n",
    "predictions = model.predict(X_test_norm)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb4022",
   "metadata": {},
   "source": [
    "applying a threshold To convert the probabilities to a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e8a7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.zeros(len(predictions))\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= 0.5:\n",
    "        y_hat[i] = 1\n",
    "    else :\n",
    "        y_hat[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8170dd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4513cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testgpu",
   "language": "python",
   "name": "testgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
